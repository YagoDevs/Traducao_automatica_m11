{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oAFeYQxc7bd"
      },
      "source": [
        "#Tradução Automática\n",
        "\n",
        "## Aluno: Yago Phellipe Matos Lopes (A2022.2A.0153:)\n",
        "### Curso: Ciência da Computação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "OPoZheI2c1sG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import hashlib\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "FuNTB4SmzyAa"
      },
      "outputs": [],
      "source": [
        "class DataModule:\n",
        "    \"\"\"Classe base que simula d2l.DataModule\"\"\"\n",
        "    def __init__(self, root='../data'):\n",
        "        self.root = root\n",
        "        os.makedirs(self.root, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ciiqHEbGkvyX"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "class TraducaoDownload(DataModule):\n",
        "    def _download(self):\n",
        "        \"\"\"Download e extração do conjunto de dados francês-inglês\"\"\"\n",
        "        # URL e hash do arquivo\n",
        "        url = 'https://d2l-data.s3-accelerate.amazonaws.com/fra-eng.zip'\n",
        "        sha1_hash = '94646ad1522d915e7b0f9296181140edcf86a4f5'\n",
        "\n",
        "        # Cria pasta data se não existir\n",
        "        os.makedirs(self.root, exist_ok=True)\n",
        "\n",
        "        # Nome do arquivo\n",
        "        fname = os.path.join(self.root, url.split('/')[-1])\n",
        "\n",
        "        # Verifica se o arquivo já existe e tem o hash correto\n",
        "        if os.path.exists(fname):\n",
        "            sha1 = hashlib.sha1()\n",
        "            with open(fname, 'rb') as f:\n",
        "                while True:\n",
        "                    data = f.read(1048576)\n",
        "                    if not data:\n",
        "                        break\n",
        "                    sha1.update(data)\n",
        "            if sha1.hexdigest() == sha1_hash:\n",
        "                print(f\"Arquivo {fname} já baixado e verificado\")\n",
        "            else:\n",
        "                print(f\"Hash do arquivo {fname} incorreto, baixando novamente\")\n",
        "                self._download_file(url, fname)\n",
        "        else:\n",
        "            print(f\"Baixando {fname} de {url}...\")\n",
        "            self._download_file(url, fname)\n",
        "\n",
        "        # Extrai arquivo\n",
        "        base_dir = os.path.dirname(fname)\n",
        "        data_dir = os.path.join(base_dir, 'fra-eng')\n",
        "        if not os.path.exists(data_dir):\n",
        "            print(f\"Extraindo {fname}...\")\n",
        "            with zipfile.ZipFile(fname, 'r') as zf:\n",
        "                zf.extractall(base_dir)\n",
        "\n",
        "        # Lê o arquivo extraído\n",
        "        with open(os.path.join(data_dir, 'fra.txt'), encoding='utf-8') as f:\n",
        "            return f.read()\n",
        "\n",
        "    def _download_file(self, url, fname):\n",
        "        \"\"\"Baixa o arquivo com progresso\"\"\"\n",
        "        try:\n",
        "            urllib.request.urlretrieve(url, fname)\n",
        "        except:\n",
        "            # Fallback para dados embutidos se download falhar\n",
        "            print(\"Erro ao baixar. Criando um arquivo de exemplo.\")\n",
        "            fallback_data = (\n",
        "                \"Go.\\tVa !\\n\"\n",
        "                \"Hi.\\tSalut !\\n\"\n",
        "                \"Run!\\tCours !\\n\"\n",
        "                \"Run!\\tCourez !\\n\"\n",
        "                \"Who?\\tQui ?\\n\"\n",
        "                \"Wow!\\tÇa alors !\\n\"\n",
        "            )\n",
        "            os.makedirs(os.path.dirname(fname), exist_ok=True)\n",
        "            with open(fname, 'w', encoding='utf-8') as f:\n",
        "                f.write(fallback_data)\n",
        "\n",
        "            # Cria diretório para extração\n",
        "            extract_dir = os.path.join(os.path.dirname(fname), 'fra-eng')\n",
        "            os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "            # Cria arquivo fra.txt\n",
        "            with open(os.path.join(extract_dir, 'fra.txt'), 'w', encoding='utf-8') as f:\n",
        "                f.write(fallback_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "bkAHTvs90FDy"
      },
      "outputs": [],
      "source": [
        "\n",
        "def preprocess_nmt(text):\n",
        "    \"\"\"Preprocessa o conjunto de dados de tradução.\"\"\"\n",
        "    def no_space(char, prev_char):\n",
        "        return char in set(',.!?') and prev_char != ' '\n",
        "\n",
        "    # Substituir espaços especiais e converter para minúsculas\n",
        "    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ').lower()\n",
        "\n",
        "    # Inserir espaço entre palavras e pontuação\n",
        "    out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char\n",
        "           for i, char in enumerate(text)]\n",
        "\n",
        "    return ''.join(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "U9OBFV5F0F8P"
      },
      "outputs": [],
      "source": [
        "def tokenize(text):\n",
        "    \"\"\"Tokenização simples por espaço.\"\"\"\n",
        "    return text.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "rQPIF85q0G_S"
      },
      "outputs": [],
      "source": [
        "def build_vocab(data, min_freq=2):\n",
        "    \"\"\"Construção do vocabulário.\"\"\"\n",
        "    counter = {}\n",
        "    for sentence in data:\n",
        "        for token in tokenize(sentence):\n",
        "            if token not in counter:\n",
        "                counter[token] = 0\n",
        "            counter[token] += 1\n",
        "\n",
        "    # Tokens especiais\n",
        "    specials = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
        "\n",
        "    # Cria vocabulário com tokens que aparecem pelo menos min_freq vezes\n",
        "    vocab = {\n",
        "        token: idx\n",
        "        for idx, token in enumerate(\n",
        "            specials +\n",
        "            [token for token, count in counter.items() if count >= min_freq]\n",
        "        )\n",
        "    }\n",
        "\n",
        "    # Dicionário reverso (para lookup rápido)\n",
        "    inv_vocab = {idx: token for token, idx in vocab.items()}\n",
        "\n",
        "    return vocab, inv_vocab, counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "UzTHQaMq0Lxt"
      },
      "outputs": [],
      "source": [
        "def process_dataset(raw_text, num_examples=None):\n",
        "    \"\"\"Processa o texto bruto em pares de sentenças.\"\"\"\n",
        "    text = preprocess_nmt(raw_text)\n",
        "    lines = text.split('\\n')\n",
        "    pairs = []\n",
        "\n",
        "    for line in lines:\n",
        "        if not line.strip():\n",
        "            continue\n",
        "        parts = line.split('\\t')\n",
        "        if len(parts) >= 2:\n",
        "            pairs.append([parts[0].strip(), parts[1].strip()])\n",
        "\n",
        "    if num_examples is not None:\n",
        "        pairs = pairs[:num_examples]\n",
        "\n",
        "    return pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TM1KfMLkxC_",
        "outputId": "b33ba2b1-2f6c-4347-e316-bc1ef087820a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Arquivo ../data/fra-eng.zip já baixado e verificado\n",
            "Amostra do texto bruto:\n",
            "Go.\tVa !\n",
            "Hi.\tSalut !\n",
            "Run!\tCours !\n",
            "Run!\tCourez !\n",
            "Who?\tQui ?\n",
            "Wow!\tÇa alors !\n",
            "\n",
            "\n",
            "Amostra do texto pré-processado:\n",
            "go .\tva !\n",
            "hi .\tsalut !\n",
            "run !\tcours !\n",
            "run !\tcourez !\n",
            "who ?\tqui ?\n",
            "wow !\tça alors !\n",
            "\n",
            "--- Análise com 1000 exemplos ---\n",
            "Total de pares: 1000\n",
            "Tamanho do vocabulário (Fonte): 266\n",
            "Tamanho do vocabulário (Alvo): 321\n",
            "\n",
            "Amostras de pares:\n",
            "Fonte: go .\n",
            "Alvo: va !\n",
            "Fonte: hi .\n",
            "Alvo: salut !\n",
            "Fonte: run !\n",
            "Alvo: cours !\n",
            "\n",
            "--- Análise com 5000 exemplos ---\n",
            "Total de pares: 5000\n",
            "Tamanho do vocabulário (Fonte): 875\n",
            "Tamanho do vocabulário (Alvo): 1230\n",
            "\n",
            "Amostras de pares:\n",
            "Fonte: go .\n",
            "Alvo: va !\n",
            "Fonte: hi .\n",
            "Alvo: salut !\n",
            "Fonte: run !\n",
            "Alvo: cours !\n",
            "\n",
            "--- Análise com 10000 exemplos ---\n",
            "Total de pares: 10000\n",
            "Tamanho do vocabulário (Fonte): 1505\n",
            "Tamanho do vocabulário (Alvo): 2250\n",
            "\n",
            "Amostras de pares:\n",
            "Fonte: go .\n",
            "Alvo: va !\n",
            "Fonte: hi .\n",
            "Alvo: salut !\n",
            "Fonte: run !\n",
            "Alvo: cours !\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    # Download e leitura dos dados\n",
        "    data = TraducaoDownload()\n",
        "    try:\n",
        "        raw_text = data._download()\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao baixar: {e}\")\n",
        "        return\n",
        "\n",
        "    print(\"Amostra do texto bruto:\")\n",
        "    print(raw_text[:75])\n",
        "\n",
        "    # Pré-processar e dividir em pares\n",
        "    text = preprocess_nmt(raw_text)\n",
        "    print(\"\\nAmostra do texto pré-processado:\")\n",
        "    print(text[:80])\n",
        "\n",
        "    # Processar o conjunto de dados com diferentes tamanhos\n",
        "    for num_examples in [1000, 5000, 10000]:\n",
        "        print(f\"\\n--- Análise com {num_examples} exemplos ---\")\n",
        "\n",
        "        # Processar pares de frases\n",
        "        pairs = process_dataset(raw_text, num_examples)\n",
        "\n",
        "        # Separar fonte e alvo\n",
        "        src_sentences = [src for src, _ in pairs]\n",
        "        tgt_sentences = [tgt for _, tgt in pairs]\n",
        "\n",
        "        # Construir vocabulários\n",
        "        vocab_src, inv_vocab_src, counter_src = build_vocab(src_sentences)\n",
        "        vocab_tgt, inv_vocab_tgt, counter_tgt = build_vocab(tgt_sentences)\n",
        "\n",
        "        # Imprimir estatísticas\n",
        "        print(f\"Total de pares: {len(pairs)}\")\n",
        "        print(f\"Tamanho do vocabulário (Fonte): {len(vocab_src)}\")\n",
        "        print(f\"Tamanho do vocabulário (Alvo): {len(vocab_tgt)}\")\n",
        "\n",
        "        # Imprimir algumas amostras\n",
        "        print(\"\\nAmostras de pares:\")\n",
        "        for i in range(min(3, len(pairs))):\n",
        "            print(f\"Fonte: {pairs[i][0]}\")\n",
        "            print(f\"Alvo: {pairs[i][1]}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "nxDrZRD9kydV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "VTvp8WfWkz-E"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "x32-GTApk1cW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "8VfXwZ9pk2l5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "lBR6xcHrk8uX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ygr4pnuak7Fk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "uVpROSuplDgx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
